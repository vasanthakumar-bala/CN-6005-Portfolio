{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMWKnoBiIv370rPH06P9LJU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IKotsSBhEV0p","executionInfo":{"status":"ok","timestamp":1766440682377,"user_tz":0,"elapsed":7098,"user":{"displayName":"Jishnu Jayaraj","userId":"06196383996260844746"}},"outputId":"8405b342-6cd9-4364-aeef-202c9ca3b8c0"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["# Import required libraries\n","import nltk\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","# Download stopwords\n","nltk.download('stopwords')\n","\n","# Sample labeled dataset\n","data = [\n","    (\"I love this product\", \"positive\"),\n","    (\"This is amazing\", \"positive\"),\n","    (\"I hate this item\", \"negative\"),\n","    (\"This is terrible\", \"negative\"),\n","    (\"I am not sure about this\", \"neutral\"),\n","    (\"This product is okay\", \"neutral\")\n","]\n","\n","# Split data into features and labels\n","X, y = zip(*data)\n","\n","# Train-test split\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# Convert text in\n"]},{"cell_type":"code","source":["# Initialize and train the vectorizer and classifier\n","vectorizer = CountVectorizer(stop_words=nltk.corpus.stopwords.words('english'))\n","X_train_counts = vectorizer.fit_transform(X_train)\n","classifier = MultinomialNB()\n","classifier.fit(X_train_counts, y_train)\n","\n","# New sentences for prediction\n","new_sentences = [\n","    \"I really love this\",\n","    \"This is the worst experience\",\n","    \"The product is average\"\n","]\n","\n","# Transform and predict\n","new_counts = vectorizer.transform(new_sentences)\n","new_predictions = classifier.predict(new_counts)\n","\n","# Display results\n","for sentence, sentiment in zip(new_sentences, new_predictions):\n","    print(f\"Sentence: {sentence}\")\n","    print(f\"Predicted Sentiment: {sentiment}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oL1sOYxmEfFr","executionInfo":{"status":"ok","timestamp":1766440751589,"user_tz":0,"elapsed":26,"user":{"displayName":"Jishnu Jayaraj","userId":"06196383996260844746"}},"outputId":"a75dc0bd-428b-4bce-d7e4-c9f5050f2c9a"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence: I really love this\n","Predicted Sentiment: negative\n","\n","Sentence: This is the worst experience\n","Predicted Sentiment: negative\n","\n","Sentence: The product is average\n","Predicted Sentiment: neutral\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"f4jV-i3wEk1G"},"execution_count":null,"outputs":[]}]}